<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>草爵的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wangld5.github.io/"/>
  <updated>2019-10-10T08:19:49.457Z</updated>
  <id>http://wangld5.github.io/</id>
  
  <author>
    <name>草爵</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NLP-learning-report1</title>
    <link href="http://wangld5.github.io/2019/10/10/NLP-learning-report1/"/>
    <id>http://wangld5.github.io/2019/10/10/NLP-learning-report1/</id>
    <published>2019-10-10T02:59:12.000Z</published>
    <updated>2019-10-10T08:19:49.457Z</updated>
    
    <content type="html"><![CDATA[<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>在NLP中，使用向量代表一个单词，词向量又被称为word embeddings。<br>获得词向量的方法包括one-hot,即建立一个one-hot矩阵表示一个词，但这样的话会导致不同的单词有不同的矩阵且他们之间没有任何联系。<br>第二种方法是使用共现矩阵，具体的实现是通过定位中心词汇和窗口大小，利用中心词汇的上下文词汇来构造一个矩阵，在这个矩阵中我们可以判断出与中心词汇频繁出现的词汇有哪些，通过SVD来对这个矩阵进行降维找到特征值相近的两个单词，那么这两个单词的语义或语境就相似。共现矩阵是统计方法.<br><img src="/.io//report-1.PNG" alt><br>第三种是word2vec，主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。CBOW是从原始语句推测目标字词，对于CBOW而言,具体过程是  </p><ol><li>为每一个单词的上下文词汇生成一个one-hot向量</li><li>通过输入词向量矩阵的权重矩阵和这些上下文词汇生成的one-hot向量的乘积</li><li>求乘积的均值.</li><li>根据均值和输出权重矩阵的乘积得到一个分数，将这个分数放进softmax函数中求得与真实值的差距。</li><li>关于权重矩阵的求法，使用SGD使一下公式的结果最小化:<br><img src="/.io//report-2.PNG" alt></li></ol><p>而Skip-Gram正好相反，是从目标字词推测出原始语句。过程和CBOW相反。</p><h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><p>完成了assignment1的共现矩阵的练习，构造了共现矩阵并测试了一组单词的相似性。完成了部分word2vec的练习。具体完成情况：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;词向量&quot;&gt;&lt;a href=&quot;#词向量&quot; class=&quot;headerlink&quot; title=&quot;词向量&quot;&gt;&lt;/a&gt;词向量&lt;/h3&gt;&lt;p&gt;在NLP中，使用向量代表一个单词，词向量又被称为word embeddings。&lt;br&gt;获得词向量的方法包括one-hot,即建立一
      
    
    </summary>
    
    
    
      <category term="NLP" scheme="http://wangld5.github.io/tags/NLP/"/>
    
  </entry>
  
</feed>
